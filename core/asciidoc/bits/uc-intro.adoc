=== Starting the Datagen connectors 

In this workshop we'll simulate the use of existing systems. To start generating some data we need to start the datagen connectors. This connectory will continuously create new data to simulate a running environment. 

Start the Anti Money Laundry Status datagen by running the following command.

[IMPORTANT]
====
[source,subs="attributes"]
----
curl -i -X POST -H "Accept:application/json" \
  -H  "Content-Type:application/json" http://localhost:18083/connectors/ \
  -d '{
    "name": "source-aml-status-change",
    "config": {
        "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "kafka.topic": "AML_Status",
        "max.interval": 1000,
        "schema.filename": "/datagen/aml_service.avro",
        "schema.keyfield": "payment_id"
      }
  }'
----
====


Start the Funds Status datagen by running the following command.

[IMPORTANT]
====
[source,subs="attributes"]
----
curl -i -X POST -H "Accept:application/json" \
  -H  "Content-Type:application/json" http://localhost:18083/connectors/ \
  -d '{
    "name": "source-funds-status-change",
    "config": {
        "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "kafka.topic": "Funds_Status",
        "max.interval": 1000,
        "schema.filename": "/datagen/funds_service.avro",
        "schema.keyfield": "payment_id"
      }
  }'
----
====

Start the Source Payment datagen by running the following command.

[IMPORTANT]
====
[source,subs="attributes"]
----
curl -i -X POST -H "Accept:application/json" \
  -H  "Content-Type:application/json" http://localhost:18083/connectors/ \
  -d '{
    "name": "source-payment-instruction",
    "config": {
        "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "kafka.topic": "Payment_Instruction",
        "max.interval": 1000,
        "schema.filename": "/datagen/payment_service.avro",
        "schema.keyfield": "payment_id"
      }
  }'
----
====


=== View Messages in Confluent Control Center

Now that the our datagen source connectors are up and running, we will be able to see messages appear in our local Kafka cluster. 

We can use link:http://{externalip}:9021[Confluent Control Center, window=_blank] to confirm this. 

Use the following and username and password to authenticate to Confluent Control Center

[source,subs="attributes"]
----
Username: {dc}
Password: your workshop password
----

image::./images/c3_05.png[]

On the landing page we can see that Confluent Control Center is monitoring two Kafka Clusters, our on-premise cluster and a Confluent Cloud Cluster

image::./images/c3_10.png[]

Click on the "controlcenter.cluster" tile, this is your on-premise cluster.

image::./images/c3_20.png[]

Select the Topics Menu on the left

image::./images/c3_30.png[]

Select the `Funds_Status` topic

image::./images/c3_40.png[]

Finally select the Messages tab and observe that messages are being streamed into Kafka in real time.

image::./images/c3_50.png[]

You can repeat these steps to check also the `AML_Status` and `Payment_Instruction` topics

.Further Reading
[TIP]
====
* link:https://debezium.io/documentation/reference/1.1/connectors/mysql.html#mysql-connector-configuration-properties_debezium[Debezium MySQL Configuration Options , window=_blank]
* link:https://docs.confluent.io/current/connect/references/restapi.html[Kafka Connect REST API]
* link:https://curl.haxx.se/docs/manpage.html[cURL manpage]
* link:https://docs.confluent.io/current/control-center/index.html[Confluent Control Center Documentation]
====


We now have `Funds_Status`, `AML_Status` and `Payment_Instruction` rows being created for us by the orders application.

